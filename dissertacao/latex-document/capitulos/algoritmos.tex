% ---
\chapter{Algoritmos de compressão sem perda}
% ---

Os algoritmos de compressão podem ser categorizadas em duas diferentes classes: os de compressão \textbf{com perda} e \textbf{sem perda}. 
Os \textbf{algoritmos de compressão com perda} admitem uma baixa porcentagem de perda de informações durante a codificação para obter maior performance, muito uteis na transmissão de dados em streaming por exemplo. 
Nos \textbf{algoritmos de compressão sem perda} o processo de codificação deve ser capaz de recuperar os dados em sua totalidade, geralmente utilizados em casos onde não pode haver perda de informações (como por exemplo, compressão de arquivos de texto).

Neste capítulo serão apresentados dois dos principais algoritmos de compressão sem perda (\textbf{Código de Huffman} e \textbf{Lempel-Ziv 77}), bem como algumas variações uteis para o propósito do presente trabalho.

\pagebreak

\section{Código de Huffman} \label{sec:huff}
O \textbf{algoritmo de Huffman} (desenvolvido por David Huffman em 1952) é um dos componentes mais utilizados em algoritmos de compressão sem perda, servindo como base para algoritmos como o Deflate (utilizado amplamente na web).
Os códigos gerados a partir do algoritmos de Huffman são chamados \textbf{Códigos de Huffman}.

O código de Huffman é descrito em termos de como ele gera uma árvore de código livre de prefixo. 
Considere o conjunto de símbolos $M$, com $p_i$ sendo a probabilidade associada a $m_i$

\begin{algorithm}[H]
\caption{Algoritmo de Huffman} \label{alg:huff}
\begin{algorithmic}
	\State $Forest \gets \emph{[]}$\\
	\ForAll{$m_i \in M$} \Comment{Inicializando floresta}
		\State $T \gets newTree()$
		\State $node \gets newNode()$
		\State $node.weight \gets p_i$ \Comment{$w_i = p_i$}
		\State $T.root \gets node$
		\State $Forest.append(T)$ \Comment{Adiciona um nova arvore na floresta}
	\EndFor \\
	
	\While{$Forest.size > 1$}
		\State $T1 \gets ExtractMin(Forest)$ \Comment{Retorna a árvore cuja raiz é mínima, e retira da floresta}
		\State $T2 \gets ExtractMin(Forest)$
		\State $HTree \gets newTree()$
		\State $HTree.root \gets newNode()$ \\
		\State $HTree.root.left \gets T1.root$
		\State $HTree.root.right \gets T2.root$
		\State $HTree.root.weight \gets T1.root.weight + T2.root.weight$
		\State Forest.append(HTree) 
	\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Análise assintótica}
Seja $n$ o tamanho do conjunto de símbolos $M$. 
Para que o algoritmo percorra toda a floresta, formada por uma árvore para cada $m \in M$, serão necessárias $n$ interações (onde $n$ é o tamanho do conjto $M$).
Considerando que as funções \emph{ExtractMin()} e \emph{.append()} foram construídas a partir de uma fila de prioridades de \textbf{heap}, o algoritmo será executado em  $O(n \log_2 n)$.

\subsection{Corretude}
O teorema a seguir (escrito por Huffman TODO REF ARTIGO) mostra  que os códigos de Huffman são ótimos e livres de prefixo.


\begin{lemma} \label{lemma:dist_prob_avg_size} Seja $C$ um código ótimo livre de prefixo, com $\{ p_1, p_2,..., p_n\}$ sendo a distribuição de probabilidades associada ao código. 
Se $p_i > p_j$ então $l(w_i) \leq l(w_j)$

\begin{proof} 
Para efeito de contradição, assuma que $l(w_i) > l(w_j)$. 
Agora construa um novo código $C'$, trocando $w_i$ por $w_j$. Dado $l_a$ como o comprimento médio do código $C$, o código $C'$ terá o seguinte comprimento:
\begin{align*}
l'_a &= l_a + p_j(l(w_i) - l(w_j)) + p_i(l(w_j) - l(w_i)) \\
&= l_a + (p_j - p_i)(l(w_i) - l(w_j)) 
\end{align*}

Pelas suposições feitas anteriormente o termo $(p_j - p_i)(l(w_i) - l(w_j))$ seria negativo, contradizendo o fato do código $C$ ser um código ótimo e livre de prefixo (pois neste caso $l'_a > l_a$).

\textbf{Nota*} : Perceba que em uma árvore de Huffman, o tamanho da palavra código $w_i$ também representa seu nível na árvore.
\end{proof}
\end{lemma}

\begin{theorem} O algoritmo de Huffman gera um código ótimo livre de prefixo.
\begin{proof}
A prova se dará por indução sobre o número de mensagens pertencentes ao código. 
Vamos mostrar que se o Algoritmo de Huffman gera um código livre de prefixo ótimo para qualquer distribuição de probabilidades com $n$ símbolos, então o mesmo ocorre para $n + 1$ símbolos.

\item \textbf{Caso Base.} Para n = 2 o teorema é trivialmente satisfeito considerando um código que atribui um bit pra cada símbolo do código.

\item \textbf{Passo indutivo}. Pelo lema \ref{lemma:dist_prob_avg_size} sabemos que as menores probabilidades estão nos menores níveis da árvore de Huffman (por ser uma árvore binária completa, o seu menor nível deve possuir ao menos dois nós). 
Já que esses nós possuiriam o mesmo tamanho, podemos muda-los de posição sem afetar o tamanho médio do código, concluindo assim que estes são nós \textbf{irmãos}.\\
Agora defina um conjunto de símbolos $M$ de tamanho $n + 1$ onde $T$ é a árvore de prefixo ótima construída a partir do Algoritmo de Huffman aplicado em $M$.
Vamos chamar os dois nós de menor probabilidade na árvore de $x$ e $y$ (que pelo argumento anterior, são nós irmãos). 
Construiremos uma nova árvore $T'$ a partir de $T$ removendo os nós $x$ e $y$, fazendo assim que o pai destes nós, que chamaremos de $z$, seja o de menor probabilidade (de acordo com a definição do Algoritmo de Huffman, $p_z = p_y + p_x$).
 Considere $k$ como a profundidade de $z$, temos:

\begin{align*}
l_a(T) &= l_a(T') + p_x(k + 1) + py(k + 1) - p_z k \\
&= l_a(T') + p_x + p_y
\end{align*}

Sabemos pela hipótese de indução que $l_a(T')$ é mínimo, pois $T'$ tem o tamanho $n$ e foi gerada pelo algoritmo de Huffman. 
Note que independente da ordem que forem inseridos, os nós $x$ e $y$ irão adicionar a constante $p_z = p_x + p_y$ no peso médio do código. 
Como $l_a(T')$ é mínimo para um conjunto de símbolos de tamanho $n$ e seu nó de menor peso tem distribuição de probabilidade $p_z$, $l_a(T)$ também é mínimo para o conjunto de símbolos $M$ e logo $T$ é ótimo e livre de prefixo. 
\end{proof}
\end{theorem}

\section{Lempel-Ziv 77 (LZ77)}

\subsection{Algoritmos de Lempel-Ziv}
Nos anos de 1977 e 1978, Jacob Ziv e Abraham Lempel publicaram dois artigos apresentando os algoritmos \textbf{LZ77} e \textbf{LZ88}, que serviriam como base para uma família de algoritmos de compressão (conforme mostrado na Figura~\ref{fig:lz77}), chamados de algoritmos de \textbf{Lempel-Ziv}.
Os algoritmos de Lempel-Ziv realizam o processo de compressão baseado em um \textbf{dicionário} de mensagens vistas anteriormente (diferente do \nameref{alg:huff}, que utiliza a probabilidade associada a cada símbolo). 
Tanto o LZ77 quanto o LZ78 tem um funcionamento parecido, que se resume em substituir partes da entrada por referências à partes iguais anteriormente processadas, e diferem na maneira em que procuram por repetições a serem substituídas. 

\begin{figure}[h]
   \centering
   \includegraphics[scale=0.75]{figs/lz77fam.png}
    \caption{Familia de algoritmos Lempel Ziv}
    \label{fig:lz77}
 \end{figure}

\subsection{Descrição do LZ77}
O LZ77 e suas variações utilizam a técnica de \emph{janela deslizante} para encontrar mensagens correspondentes. 
A janela é dividida em duas partes separadas por um \emph{cursor} (que se move conforme novas mensagens são codificadas).

A mensagem à esquerda do cursor é chamada de \textbf{dicionário}, e contém todos os símbolos já codificadas. Já a mensagem à direita do \emph{cursor} é chamada de \textbf{lookahead buffer}.
A ideia geral do algoritmo é substituir as mensagens do \emph{lookahead buffer} por \textbf{tokens}, onde cada \emph{token} é constituído por uma \emph{tupla} que ``aponta''  para a maior mensagem correspondente a cadeia no início do \emph{lookahead buffer}.

A \emph{tupla} que constitui os \emph{tokens} é formada por três valores:
Um valor inteiro que indica quantas posições para trás o \emph{cursor} deve retornar até encontrar o início da cadeia, 
um segundo valor inteiro indicando o tamanho da cadeia,
e um caracter (preenchido com $m_{cursor}$ ou $null$ para \textbf{tokens não vazios}).

Os \emph{tolkens} podem ser vazios, quando não encontramos nenhuma mensagem correspondente, neste caso, os dois primeiros valores da \emph{tupla} são preenchidos com zeros.
Por exemplo, se durante a compressão de um texto encontrássemos o símbolo ``a'' pela primeira vez (isso é, não existe correspondência pra ele no \emph{dicionário}), a saída seria o \emph{token vazio} $(0, 0, ``a'')$.

\begin{algorithm}[H]
\caption{Algoritmo Lempel-Ziv 77} \label{alg:lz77}
\begin{algorithmic}

	\State $cursor \gets 0$
	\State $tokens \gets []$

	\While{$cursor < M.size()$}
		\State $position, len \gets findLongestMatch(M, cursor)$ \Comment{Retorna a posição e tamanho da maior cadeia correspondente}
		\If{$len > 0$} \Comment{Cria novo token, que aponta para a cadeia encontrada}
			\State $tokens.append([position, len, null])$
			\State $cursor \gets cursor + len$
		\Else
			\State $tokens.append((0, 0, M[cursor]))$ \Comment{Token vazio}
			\State $cursor \gets cursor + 1$
		\EndIf
	\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Função ``\emph{findLongestMatch}'' }
O \emph{LZ77} possui um processo de decodificação muito eficiente, porém consome muitos recursos computacionais na codificação. 
Isto é, ele é mais eficiente para casos onde pretende-se decodificar o arquivo múltiplas vezes, ou ainda decodifica-lo numa máquina com menor poder computacional. 
O ``gargalo'' da compressão está na técnica de busca pelo \emph{dicionário} para encontrar a mensagem correspondentes mais longa.

O método convencional utilizado é a \textbf{busca linear}, que consiste em comparar cada posição no dicionário com o \emph{lookahead buffer} e selecionar a maior correspondência encontrada. 
Tome $N$ como o tamanho do dicionário e $M$ como o tamanho do \emph{lookahead buffer}, no pior caso, a busca linear é realizada em tempo $O(NM)$. 
A busca será executada $M + N$ vezes, isto é, a complexidade geral do algoritmo é $O( N^{2}M + M^{2}N)$.

\subsection{Melhorias na performance da função ``\emph{findLongestMatch}'' }

Uma das possíveis abordagens para melhorar a busca linear é utilizar estruturas de dados auxiliares, que indexam os símbolos do dicionário para tornar a busca mais rápida.
(TODO: Citacao do artigo) testa diferentes estruturas de dados (arvores trie, hash tables, arvores binárias de busca entre outros). 
Para este projeto utilizaremos a \textbf{lista ligada} como estrutura de dados auxiliar (principalmente por possuir uma implementação mais simples). 

Para construir uma estrutura de dados auxiliar com lista ligada, podemos criar uma lista para cada símbolo no dicionário, que contenha os indicies onde o símbolo pode ser encontrado.  
Para limitar a janela de busca, podemos construir a lista como uma \emph{fila}, onde ao atingirmos a capacidade máxima da janela de busca, removemos o menor indicie (que estaria no inicio da fila). 
Note que, o tamanho máximo da lista depende da quantidade de bits que queremos utilizar para construir o \emph{token}.

Encontramos a maior mensagem correspondente, comparando o \emph{lookahead buffer} com as cadeias de símbolos iniciadas a partir de indicies contidos na lista que corresponde ao primeiro símbolo do \emph{lookahead buffer}.
Neste contexto, o pior caso ocorrerá quando todos os símbolos no dicionário forem iguais, pois todos as posições no dicionário seriam checadas (levando a uma performance equivalente a da busca linear).
Entretanto, para uma base de dados de texto real com grande quantidade de dados isso raramente aconteceria, e apenas uma quantidade $K \ll N$ de possíveis mensagens seriam checadas.

\section{Compressão baseada em palavras}
A implementação padrão para a maioria dos algoritmos de compressão utiliza como \emph{alfabeto de origem} elementos de 8 bits (também conhecidos como \textbf{\emph{caracteres}}).
Tal método limita a correlação de cadeias mais longas, e tem sua eficiência limitada quando aplicado a grande quantidade de dados. 
Em contrapartida, se definirmos cada símbolo do \emph{alfabeto de origem} como uma sequência caracteres, poderíamos tirar vantagem de longas correspondências e talvez obter melhores resultados na compressão.

Quando a natureza dos dados é previamente conhecida, podemos tomar vantagem da sua estrutura para definir os símbolos de uma maneira mais eficiente. 
Em especial, linguagens ``faladas'' possuem uma estrutura hierárquica bem definida, que vai desde o agrupamento de letras em sílabas, silabas em palavras, palavras em frases e assim por diante. 

Neste contexto, definiremos uma \textbf{\emph{palavra}} como uma sequência maximal de \emph{caracteres} alfanuméricos, divididos por sequências de caracteres não alfanuméricos chamados \emph{separadores} (baseado na definição feita por Bentley et al. TODO referencia).
A compressão baseada em palavras é uma modificação dos algoritmos de compressão clássicos, que considera cada palavra como um símbolo (i.e, o \emph{alfabeto de origem} passa a ser composto por \emph{palavras}, não por \emph{caracteres}).

\subsection{Huffword} \label{sub:huffw}
O método de compressão \emph{Huffword} é uma modificação da implementação canônica do \emph{código de Huffman}. 
Desenvolvido por Moffat e Zobel (Incluir referencia) em 1994, ele utiliza \emph{palavras} em seu alfabeto de origem.

O algoritmo funciona de maneira bem similar à implementação canônica, o arquivo é pré-processado separando as \emph{palavras} e os \emph{separadores} como duas entradas distintas.
Depois o algoritmo de Huffman canônico é aplicado a cada uma das partes, sendo que as probabilidades agora estão associadas as palavras e não mais aos caracteres. 
A codificação associada a entrada também é armazenada em duas \emph{strings} distintas, assim como as tabelas de códigos também são destintas.
 
 \subsection{WLZ77}
 (TODO REF ARTIGO) implementa a versão do LZ77 baseado em palavras (também chamado WLZ77) aplicando uma ideia semelhante à apresentada anteriormente . 
 O arquivo é pré-processado para obter uma lista de \emph{palavras} e \emph{separadores}. 
 Ao contrário do ~\nameref{sub:huffw} as palavras e separadores são processados simultaneamente pela implementação canônica do LZ77, gerando uma sequência única de \emph{tokens}.
 
 Vale notar que, neste caso, o último elemento da \emph{tupla} no token não é mais um caractere e sim uma palavra. Isso significa que para o caso dos \emph{tokens vazios}, o tamanho do \emph{token} irá variar de acordo com o tamanho da palavra.
 
 
 % ---
 \chapter{Clusterização de dados aplicada a textos}
 % ---
 
 O processo de clusterização de dados consiste em agrupar os objetos (que compõe o conjunto de dados) em $n$ diferentes \textbf{agrupamentos} (clusters), de maneira em que os objetos de um mesmo grupo sejam similares e os de grupos diferentes dissimilares.
 Para isto, faz-se essencial uma definição clara da similaridade entre os objetos, que pode variar dependendo da natureza do dado em análise.
 
 Podemos construir o processo de clusterização de diferentes maneiras: Na \textbf{clusterização sem sobreposição} cada objeto deve pertencer a exatamente um cluster. 
 Em contrapartida podemos agrupar os objetos permitindo algumas sobreposições. 
 Podemos ainda clusterizar dados por \textbf{hierarquia}, onde os dados são organizados em níveis (quase como uma árvore).
 
 \begin{figure}[H]
   \centering
   \includegraphics[scale=0.50]{figs/cluster_h.png}
    \caption{Exemplo de agrupamento hierárquico}
    \label{fig:clusterh}
 \end{figure}
 
 \pagebreak
 
 \section{Similaridade em dados textuais}
 Conforme introduzido anteriormente, definir uma métrica de similaridade entre os objetos é essencial para o funcionamento dos algoritmos de clusterização.
 Para o propósito deste projeto, iremos explorar técnicas que extraem essa similaridades em dados textuais.
 
 Para uma base dados textual,  utilizaremos o termo \textbf{``texto bruto''}  para se referir ao conteúdo de cada registro na base de dados. 
 Também adotaremos o termo \textbf{documento} para partições dos dados (por exemplo, uma linha da tabela).
 Estenderemos a definição de \emph{palavras} e \emph{separadores} utilizadas no capítulo anterior, definidas sob um ponto de vista linguístico (não relacionados diretamente a notações de conjuntos definida no capítulo ~\ref{chapter:fund}).
 
 \subsection{Vetorização TF-IDF}
Analise de texto é umas das aplicação mais exploradas no campo dos sistemas inteligentes.
Porém, pode ser uma tarefa difícil utilizar texto bruto como entrada para os algoritmos de \emph{machine learning}. 
Geralmente, tais algoritmos ``lidam melhor'' com características numéricas.
 
O processo de transformar documentos em vetores de características numéricas é conhecido como \textbf{vetorização}. 
Em geral, procuramos uma medida que atribua pesos a cada palavra, de acordo com a sua ``importância'' no documento.

Poderíamos atribuir pesos as palavras de maneira direta e intuitiva, atrelando o peso diretamente a frequência em que aquela palavra ocorre em um documento.
Chamamos esta medida de \textbf{TF} (\emph{term frequency}).

\begin{equation}\label{eq:tf}
tf(t, d) = \frac{count(t \in d)}{count(d)}
\end{equation}

Onde $t$ é a palavra (ou termo) alvo da medição, e $d$ é o documento que contém a palavra $t$  e pertence a coleção de documentos $D$.

Entretanto, algumas palavras (como ``and'' ou ``him'' no inglês) adicionam pouca ou nenhuma informação relevante para o texto em si, apesar de sua alta frequência, essas palavras são conhecidas como \textbf{\emph{stop words}}.
Remover as \emph{stop words} pode diminuir ruídos nos dados utilizados para o modelo de classificação, sendo um recurso muito importante para a construção de clusters com maior significado semântico.

Para compensar este fator, o \textbf{IDF} (\emph{inverse document frequency}) pondera o quão ``incomum'' é um determinado termo entre diferentes documentos, atribuindo valores baixos a termos que se repetem muito em diferentes documentos.

\begin{equation}\label{eq:idf}
idf(t, D) = \log \frac{len(D)}{count(d \in D : t \in d)}
\end{equation}


A métrica \textbf{TF-IDF} multiplica as equações~\ref{eq:idf} e ~\ref{eq:tf}, de maneira que a informação da frequência de um termo é ponderada pela sua ``exclusividade'' entre os documentos.
Obtemos assim uma métrica que aproxima a zero os símbolos menos relevantes, e atribuí valores maiores a símbolos ``mais relevantes''.

\begin{equation}
tfidf(t, d, D) = tf(t, d) \times idf(t, D)
\end{equation}

 \section{Redução de Dimensionalidade}
 Em casos em que a base de dados possuem uma quantidade elevadas de dimensões, 
reduzir a dimensionalidade significa procurar variáveis que possuem dependência entre si.
No geral, busca-se manter apenas características relevantes aos dados, reduzindo sua complexidade e o poder computacional necessário para processar estes dados.

 \subsection{PCA}
 PCA (ou \emph{principal component analysis}) é formada por um procedimento algébrico, 
 que reduz as variáveis correlacionadas em um conjunto de variáveis não relacionadas (linearmente) chamadas \emph{principal components} (PCs), iterativamente.
 
No contexto da clusterização, o PCA é utilizado principalmente para transformar dados com muitas dimensões em uma representação com menores dimensões (usualmente duas dimensões, facilitando a análise gráfica dos dados).
 
 
 \section{Clusterização K-means}
O método de clusterização \emph{k-means} (ou algoritmo de Lloyd's) é um algoritmo iterativo que particiona os dados em $k$ clusters, definidos por \textbf{pontos centrais} (onde $k$ é uma das entradas para o algoritmo).
A ideia em geral é encontrar os melhores $k$ pontos centrais, isto é, posicionar estes pontos de maneira a minimizar a distância dos dados aos pontos centrais mais próximos. 
Podemos definir o k-means da seguinte forma:
 \begin{enumerate}
 	\item Escolha os $k$ primeiros pontos centrais. A escolha pode ser \textbf{aleatória}, ou utilizando o \emph{k-means++} (algoritmo para inicializar os pontos centrais).
	\item Calcule a distância de cada dado em relação a cada ponto central.
	\item Atribua cada dado ao ponto central mais próximo.
	\item Calcule a distância média entre os pontos e seus respectivos centros para cada cluster, e obtenha novas localizações para os pontos centrais (de maneira a minimizar a distância média).
	\item Repita os passos 2, 3, e 4 até que os clusters não mudem, ou atingir o número máximo de iterações.
\end{enumerate}
 
 \begin{figure}[H]
   \centering
   \includegraphics[scale=0.40]{figs/kmeans.png}
    \caption{Exemplo de clusterzacao k-means}
    \label{fig:kmeans}
 \end{figure}
 
 
 \subsection{Método de elbow}
 O resultado do \emph{k-means} é diretamente afetado pelo número de clusters definidos na inicialização do algoritmo.
 Existem diversas formas para determinar o número ``ideal'' de clusters, uma delas é o \textbf{método de elbow}.
 
 Neste método, executa-se o \emph{k-means} para um range pré-definido de valores em $k$. 
 Depois disso, calcula-se a soma das distâncias quadráticas (também chamada de \textbf{inércia}) de cada ponto para o seu respectivo ponto central.
 0 resultado para cada $k$ é plotado em um gráfico de linha (conforme mostra a figura~\ref{fig:elbow}).
 Seleciona-se o número de clusters que corresponde ao ``cotovelo'' (elbow) do gráfico.
 
\begin{figure}[H]
   \centering
   \includegraphics[scale=0.40]{figs/elbowex.png}
    \caption{Método de elbow}
    \label{fig:elbow}
\end{figure}
 
 


