% ---
\chapter{Conceitos e definições fundamentais}
% ---
Este capítulo apresenta algumas definições e conceitos fundametais para o entendimento das técnicas de compressão que serão discutidas em capítulos posteriores.

\section{Código}

Um \textbf{código} \emph{C} mapeia uma \textbf{mensagem} \emph{m} $\in$ \emph{M} para uma cadeia de \textbf{palavras código} em \emph{$W^+$},onde \emph{M} é chamado \textbf{alfabeto de origem} e \emph{$W^+$} \textbf{alfabeto de palavras código}. Vamos utilizar a notação $\emph{A}^+$ para se referir ao conjunto que contém todas as cadeias de \emph{A}, \emph{i.e}, $\emph{A}^+ = \bigcup_{i \geq 1}^{}A^{i} : A^i = (a_1,...,a_i), a \in A $. Deste modo, podemos representar um código como uma função \textbf{\emph{C} : $\emph{M} \rightarrow \emph{W}^+$}.  O \textbf{comprimento} da palavra código \emph{w}, definido pela função \emph{l}(\emph{w}), representa o número de bits de \emph{w}

Os elementos dos alfabetos de origem e de palavras código podem ter um comprimento fixo ou variável. Códigos nos quais os alfabetos possuem um comprimento fixo são chamados de \textbf{códigos de comprimento fixo}, enquanto os que possuem alfabetos de comprimento variáveis são chamados \textbf{códigos de comprimento variável}. Provavelmente o exemplo mais conhecido de código de comprimento fixo seja código ASCII, que mapeia 64 símbolos alfa-numéricos (ou 256 em sua versão estendida) para palavras código de 8 bits. Todavia, a compressão de dados utiliza apenas códigos de comprimento variável, mas especificamente códigos que variam o comprimento de acordo com a probabilidade associada à mensagem (o tema será melhor detalhado em seções posteriores). 

\subsection{Códigos unicamente decodificáveis e livres de prefixo}
Um código é \textbf{distinto} se pode ser representado como uma função \textbf{bijetora}, i.e, $\forall$ $m_1$, $m_2$ $\in$ M, \emph{C($m_1$)} $\neq$ \emph{C($m_2$)}. Um código é dito \textbf{unicamente decodificável} quando \emph{C(m)} = $w$ $\leftrightarrow$ \emph{$C^{-1}(w)$} = \emph{m}, com \emph{m} $\in$ \emph{M} e \emph{$w$} $\in$ $W^+$.

Vamos definir \emph{$C^+$} como a \textbf{codificação} correspondente ao código \emph{C}, tal que $C^+(m^n) = C(m_1)C(m_2)...C(m_n) : m^n = m_1m_2...m_n$, \emph{i.e}, \emph{$C^+ : M^+ \rightarrow W^+$}. A função de \textbf{decodificação} \emph{$D^+ : W^+ \rightarrow M^+$} se refere a operação inversa da codificação, de modo que dado um código \textbf{unicamente decodificável} \emph{C}, $D^+(C^+(m^n)) = m^n$.

Um \textbf{código livre de prefixo} é um código \emph{C'} em que $\nexists w_1, w_2 \in W^+$ $|$ $w_1$ é \textbf{prefixo} de $w_2$, por exemplo, o conjunto de palavras código \emph{$W^+$} := $\{1, 01, 000, 001\}$ não possui nenhuma cadeia que é prefixo de outra dentro do conjunto. Códigos livres de prefixo podem ser \emph{decodificados instantaneamente}, ou seja, podemos decodificar uma palavra código sem precisar verificar o início da seguinte.

Um código livre de prefixo pode ser modelado por uma arvore binária. Imagine que cada mensagem $\emph{m} \in \emph{M}$ é uma folha. A palavra código \emph{C'(m)} é o caminho \emph{p} da raiz até a folha correspondente a \emph{m}, de maneira em que, para cada nó percorrido concatene um bit à \emph{p} (0 quando o nó está à esquerda e 1 quando está à direita). Chamamos tal árvore de \textbf{árvore do código livre de prefixo}.

\begin{theorem}
Todo código \textbf{livre de prefixo} é \textbf{unicamente decodificável}.

\begin{proof}
Seja C um código livre de prefixo e $S_n$ = $s_1...s_n$ uma mensagem codificada por C. Vamos provar por indução que o teorema é verdadeiro para todo n $\in$ $\mathbb{Z+}$

\item \textbf{Casos base}: Quando n = 1, a mensagem S só possui uma palavra código, logo é unicamente decodificável. Se n = 2, então S possui uma palavra código $s_1$ que não pode ser prefixo de $s_2$ (pela própria definição de códigos livres de prefixo), o que claramente significa que S é unicamente decodificável.

\item \textbf{Passo indutivo}: Seja k $\in$ $\mathbb{Z+}$, e suponha por hipótese de indução que o teorema vale para n $\leq$ k. Como $S_{k+1}$ é livre de prefixo, existe um prefixo de $S_{k+1}$, $S_j$ = $s_1...s_j$ (com j $\leq$ k + 1) que é unicamente decodificável (dado que ela não pode ser prefixo de nenhuma outra). a mensagem $S'_{k+1}$ = $s_{j+1}...s_{k+1}$ ainda é uma concatenação decodificável e |$S'_{k+1}$| $\leq$ |$S_{k+1}$|, o que significa que por hipótese de indução $S'_{k+1}$ é unicamente decodificável. Como $S_{k+1}$ = $S_j$ $S'_{k+1}$, segue que $S_{k+1}$ é unicamente decodificável.
\end{proof}
\end{theorem}

\section{Relações fundamentais com a Teoria da Informação}
A codificação é comumente divida em duas componentes diferentes: \emph{modelo} e \emph{codificador}. O \emph{modelo} identifica a distribuição de probabilidade das mensagens baseado em sua semântica e estrutura. O \emph{codificador} toma vantagem de um possível \emph{bias} apontado pela modelagem, e usa uma estratégia gulosa em relação a probabilidade associada às mensagens para reduzir seu tamanho. (substituindo as mensagens que ocorrem com maior frequência por símbolos menores).

Desta forma, é evidente que os algoritmos de compressão sempre devem tomar vantagem de alguma distribuição de probabilidades "desbalanceada" sobre as mensagens para efetivamente reduzir o tamanho destas, portanto, a compressão é fortemente relacionada com a probabilidade. Nesta seção, vamos construir o embasamento teórico necessário para entender a relação entre as probabilidades associadas e o comprimento das mensagens, e consequentemente criar uma noção dos parâmetros que devem ser maximizados para alcançar uma codificação eficiente.

\subsection{Distribuição de Probabilidade e Esperança}
Dado um experimento e um espaço amostral $\Omega$, uma \textbf{variável aleatória} \emph{X} associa um número real a cada um dos possíveis resultados em $\Omega$. Em outras palavras, \emph{X} é uma função que mapeia os elementos do espaço amostral para números reais. Quando a imagem de \emph{X} pode assumir um número finito de valores, dizemos que \emph{X} é uma \textbf{variável aleatória discreta}.

Podemos descrever melhor uma variável aleatória, atribuindo probabilidades sobre os valores que esta pode assumir. Esses valores são atribuídos pela \textbf{função de densidade de probabilidade}, denotada por \emph{$p_X$}. Portanto, a probabilidade do evento \{ \emph{X} = \emph{x} \} é a função de distribuição de probabilidade aplicada a x, \emph{i.e}, \emph{$p_X(x)$}.
\begin{equation} \label{eq:dist_prob_def}
p_X(x) = P(\{X = x\})
\end{equation}

Note que, a variável aleatória pode assumir qualquer um dos valores no espaço amostral que possuem uma probabilidade $\emph{P} > 0$, portanto
\begin{equation} \label{eq:dist_prob_sum}
\sum_{x}^{}p_X(x) = 1.
\end{equation}

O \textbf{valor esperado} (ou \textbf{esperança}) da variável aleatória \emph{X} é definido como
\begin{equation} \label{eq:exp_val}
\textbf{E}[\ X]\ = \sum_{x}^{} xp_X(x).
\end{equation}

\subsection{Comprimento médio do código}
Seja \emph{p} a distribuição de probabilidade associada ao alfabeto de origem \emph{M}. Assuma que \emph{C} é um código tal que \emph{C(m)} = \emph{w}, definimos o \textbf{tamanho médio} de \emph{C} como:
\begin{equation} \label{eq:code_len}
l_a (C) = \sum_{m \in M, w \in W^+}^{} p(m) l(w)
\end{equation}

Um código \emph{C} livre de prefixo é \textbf{ótimo} se $l_a(C)$ é mínimo, isto é, para qualquer código livre de prefixo \emph{C'} temos que
\begin{equation} \label{eq:code_len_optimal}
l_a(C) \leq l_a(C')
\end{equation}

\subsection{Entropia}
A \textbf{Entropia de Shannon} aplica as noções de Entropia física (que representa a aleatoriedade de um sistema) à Teoria da Informação. Dado um sistema \emph{S} e a função \emph{p} sendo a distribuição de probabilidade associada a \emph{S}, definimos \textbf{Entropia} como:
\begin{equation} \label{eq:entropy}
H(S) = \sum_{s \in S}^{} p(s) \log_2 \frac{1}{p(s)}
\end{equation}

Por esta definição temos que quanto menor o \emph{bias} da função de distribuição de probabilidade relacionada ao sistema, maior a sua entropia. Em outras palavras, a entropia de um sistema esta intimamente ligada a sua  "desordem". 

Shannon (incluir referencia papper do shannon) aplica o mesmo conceito de entropia no contexto da teoria da informação, "substituindo" o conjunto de estados \emph{S} pelo conjunto de mensagem \emph{M}, isto é, \emph{M} é interpretado como um conjunto de possíveis mensagens, tendo como \emph{p(m)} a probabilidade de $m \in M$.
Baseado na mesma premissa, Shannon mede a informação contida em uma mensagem da seguinte forma:
\begin{equation} \label{label:info_quantity}
\emph{i(s)} = \log_2 \frac{1}{p(s)}.
\end{equation}

\subsection{Comprimento de Código e Entropia}
Nas secções anteriores, o comprimento médio de um código  foi definido em função da distribuição de probabilidade associada ao seu alfabeto de origem.
Da mesma forma, as noções de \textbf{Entropia} relacionada a um conjunto de mensagens, tem ligação direta com as probabilidades associadas a estas. 
A seguir, será mostrado como podemos relacionar o comprimento médio de um código a sua entropia através da \textbf{Desigualdade de Kraft-McMillan} , e por consequência estabelecer uma relação direta entre \textbf{a Entropia de um conjunto de mensagens e a otimalidade do código associada a estas mensagens}.

\begin{lemma}[Desigualdade de Kraft-McMillan]  \textbf{Parte (a) McMillan} . Para todo código unicamente decodificável \textbf{\emph{C} : $\emph{M} \rightarrow \emph{W}^+$} .
\begin{equation*}
\sum_{w \in W^+}^{}2^{-l(w)} \leq 1.
\end{equation*}

\textbf{Parte (b) Kraft}. Para qualquer conjunto \emph{L} de comprimento códigos que satisfaça:
\begin{equation*}
\sum_{l \in L}^{} 2^{-l} \leq 1.
\end{equation*}
$\exists C$ livre de contexto : $ |\emph{C(w)}| =  \emph{l(w)} ~,~\forall w \in W^+$.

\begin{proof}
\end{proof}
\end{lemma}

\begin{lemma}[Entropia como limite inferior para o comprimento médio] Dado um conjunto de mensagens \emph{M} associado a uma distribuição de probabilidades \emph{p} e um código unicamente decodificável \emph{C}.
\begin{equation*}
H(M) \leq l_a(C)
\end{equation*}

\begin{proof}
Queremos provar que $H(M) - l_a(C) \leq 0$, dado que  $H(M) \leq l_a(C) \Leftrightarrow H(M) - l_a(C) \leq 0$.

Substituindo a equação~\ref{eq:entropy} em \emph{H(M)} e~\ref{eq:code_len} em \emph{$l_a(C)$}, temos:

\begin{align*}
H(M) - l_a(C) &= \sum_{m \in M}^{}p(s) \log_2 \frac{1}{p(m)}  - \sum_{m \in M, w \in W^+}^{}p(m) l(w) \\
&= \sum_{m \in M, w \in W^+}^{}p(m) \left(  \log_2 \frac{1}{p(m)} - l(w) \right) \\
&= \sum_{m \in M, w \in W^+}^{}p(m) \left(  \log_2 \frac{1}{p(m)} - \log_2 2^{l(w)} \right) \\
&= \sum_{m \in M, w \in W^+}^{}p(m) \log_2 \frac{2^{-l(w)}}{p(m)}
\end{align*}

A \textbf{Desigualdade de Jansen} afirma que se uma função \emph{f(x)} é côncava, então $\sum_{i}{}p_i~f(x_i) \leq f(\sum_{i}{}p_i~x_i)$. Como a função $\log_2$ é côncava, podemos aplicar a Desigualdade de Jansen ao resultado obtido anteriormente.

\begin{equation*}
\sum_{m \in M, w \in W^+}^{}p(m) \log_2 \frac{2^{-l(w)}}{p(m)}  \leq \log_2(\sum_{m \in M, w \in W^+}{}2^{-l(w)})
\end{equation*}

Agora aplicamos a desigualdade de Kraft-McMillan, e concluímos que:

\begin{equation*}
H(M) - l_a(C) \leq \log_2(\sum_{m \in M, w \in W^+}{}2^{-l(w)}) \Rightarrow H(M) - l_a(C) \leq 0.
\end{equation*}


\end{proof}

\end{lemma}

\begin{lemma}[Entropia como limite superior para o comprimento médio de um código livre de prefixo ótimo] Dado um conjunto de mensagens \emph{M} associado a uma distribuição de probabilidades \emph{p} e um código livre de prefixo ótimo \emph{C}.
\begin{equation*}
l_a(C) \leq H(M) + 1
\end{equation*}

\begin{proof}
$\left \lceil{x}\right \rceil $
\end{proof}
\end{lemma}

\section{Códigos de Huffman}
O \textbf{algoritmo de Huffman} (desenvolvido por David Huffman em 1952) é um dos componentes mais utilizados em algoritmos de compressão sem perda, servindo como base para algoritmos como o GZIP (utilizado amplamente na web).
Os códigos gerados a partir do algoritmos de Huffman são chamados \textbf{Códigos de Huffman}.

O código de Huffman é descrito em termos de como ele gera uma árvore de código livre de prefixo. Considere o conjunto de mensagens \emph{M}, com $p_i$ sendo a probabilidade associada a $m_i$

\begin{itemize}
	\item Crie uma floresta de árvores uma para cada mensagem do código. Faça com que o peso de cada vértice seja $w_i = p_i$ (onde o $p_i$ representa a probabilidade associada à $m_i$).
	\item Faça até que a floresta possua uma única árvore
	\begin{itemize}
		\item Selecione duas árvores com os pesos $w_1$ e $w_2$, de maneira que estes sejam os menores pesos.
		\item Crie uma nova árvore a partir das duas selecionadas anteriormente, onde a raiz é um novo nó com o peso $w_1 + w_2$ e faça das duas árvore selecionadas os seus filhos. (por convenção o nó com menor peso fica à esquerda)
	\end{itemize}
\end{itemize}

\begin{algorithm}
\caption{Algoritmo de Huffman} \label{alg:huff}
\begin{algorithmic}
	\State $Forest \gets \emph{[]}$\\
	\ForAll{$m_i \in M$} \Comment{Inicializando floresta}
		\State $T \gets newTree()$
		\State $node \gets newNode()$
		\State $node.weight \gets p_i$ \Comment{$w_i = p_i$}
		\State $T.root \gets node$
		\State $Forest.append(T)$
	\EndFor \\
	
	\While{$Forest.size > 1$}
		\State $T1 \gets ExtractMin(Forest)$ \Comment{Extrai a árvore cujo o peso da raiz é mínimo}
		\State $T2 \gets ExtractMin(Forest)$
		\State $HTree \gets newTree()$
		\State $HTree.root \gets newNode()$ \\
		\State $HTree.root.left \gets T1.root$
		\State $HTree.root.right \gets T2.root$
		\State $HTree.root.weight \gets T1.root.weight + T2.root.weight$
		\State Forrest.append(HTree) 
	\EndWhile
	
\end{algorithmic}
\end{algorithm}

\subsection{Análise Assintótica}
Seja \emph{n} o tamanho do conjunto de mensagens \emph{M}. Para que o algoritmo percorra toda a floresta, formada por uma árvore para cada $\emph{m} \in \emph{M}$, serão necessárias \emph{n} interações.
Considerando que as funções \emph{ExtractMin()} e \emph{.append()} foram construídas a partir de uma fila de prioridades de \textbf{heap}, o algoritmo será executado em  $O(n \log_2 n)$.

\subsection{Correctude}
O teorema a seguir (escrito por Huffman) mostra a principal propriedade do Algoritmo de Huffman, os códigos de Huffman são códigos ótimos e livres de prefixo.

\begin{theorem} O algoritmo de Huffman gera um código ótimo livre de prefixo.
\begin{proof}
\end{proof}
\end{theorem}

% --- Guardando para exemplo
% A formatação das referências bibliográficas conforme as regras da ABNT são um
% dos principais objetivos do \abnTeX. Consulte os manuais
% \citeonline{abntex2cite} e \citeonline{abntex2cite-alf} para obter informações
% sobre como utilizar as referências bibliográficas.

% %-
% \subsection{Acentuação de referências bibliográficas}
% %-

% Normalmente não há problemas em usar caracteres acentuados em arquivos
% bibliográficos (\texttt{*.bib}). Na~\autoref{tabela-acentos} você encontra alguns exemplos das conversões mais importantes. Preste atenção especial para `ç' e `í'
% que devem estar envoltos em chaves. A regra geral é sempre usar a acentuação
% neste modo quando houver conversão para letras maiúsculas.

% \begin{table}[htbp]
% \caption{Tabela de conversão de acentuação.}
% \label{tabela-acentos}

% \begin{center}
% \begin{tabular}{ll}\hline\hline
% acento & \textsf{bibtex}\\
% à á ã & \verb+\`a+ \verb+\'a+ \verb+\~a+\\
% í & \verb+{\'\i}+\\
% ç & \verb+{\c c}+\\
% \hline\hline
% \end{tabular}
% \end{center}
% \end{table}


% ---
% \section{Deu pau em algo?}
% ---

% Consulte a FAQ com perguntas frequentes e comuns no portal do \abnTeX:
% \url{https://code.google.com/p/abntex2/wiki/FAQ}.

% Inscreva-se no grupo de usuários \LaTeX:
% \url{http://groups.google.com/group/latex-br}, tire suas dúvidas e ajude a galera se tiver tudo certo.


