% ---
\chapter{Conceitos e definições fundamentais}
% ---
Este capítulo apresenta algumas definições e conceitos fundametais para o entendimento das técnicas de compressão que serão discutidas em capítulos posteriores.

\section{Código}

Um \textbf{código} \emph{C} mapeia uma \textbf{mensagem} \emph{m} $\in$ \emph{M} para uma cadeia de \textbf{palavras código} em \emph{$W^+$},onde \emph{M} é chamado \textbf{alfabeto de origem} e \emph{$W^+$} \textbf{alfabeto de palavras código}. Vamos utilizar a notação $\emph{A}^+$ para se referir ao conjunto que contém todas as cadeias de \emph{A}, \emph{i.e}, $\emph{A}^+ = \bigcup_{i \geq 1}^{}A^{i} : A^i = (a_1,...,a_i), a \in A $. Deste modo, podemos representar um código como uma função \textbf{\emph{C} : $\emph{M} \rightarrow \emph{W}^+$}.  O \textbf{comprimento} da palavra código \emph{w}, definido pela função \emph{l}(\emph{w}), representa o número de bits de \emph{w}

Os elementos dos alfabetos de origem e de palavras código podem ter um comprimento fixo ou variável. Códigos nos quais os alfabetos possuem um comprimento fixo são chamados de \textbf{códigos de comprimento fixo}, enquanto os que possuem alfabetos de comprimento variáveis são chamados \textbf{códigos de comprimento variável}. Provavelmente o exemplo mais conhecido de código de comprimento fixo seja código ASCII, que mapeia 64 símbolos alfa-numéricos (ou 256 em sua versão estendida) para palavras código de 8 bits. Todavia, a compressão de dados utiliza apenas códigos de comprimento variável, mas especificamente códigos que variam o comprimento de acordo com a probabilidade associada à mensagem (o tema será melhor detalhado em seções posteriores). 

\subsection{Códigos unicamente decodificáveis e livres de prefixo}
Um código é \textbf{distinto} se pode ser representado como uma função \textbf{bijetora}, i.e, $\forall$ $m_1$, $m_2$ $\in$ M, \emph{C($m_1$)} $\neq$ \emph{C($m_2$)}. Um código é dito \textbf{unicamente decodificável} quando \emph{C(m)} = $w^n$ $\leftrightarrow$ \emph{$C^{-1}(w^n)$} = \emph{m}, com \emph{m} $\in$ \emph{M} e \emph{$w^n$} $\in$ $W^+$.

Vamos definir \emph{$C^+$} como a \textbf{codificação} correspondente ao código \emph{C}, tal que $C^+(m^n) = C(m_1)C(m_2)...C(m_n) : m^n = m_1m_2...m_n$, \emph{i.e}, \emph{$C^+ : M^+ \rightarrow W^+$}. A função de \textbf{decodificação} \emph{$D^+ : W^+ \rightarrow M^+$} se refere a operação inversa da codificação, de modo que dado um código \textbf{unicamente decodificável} \emph{C}, $D^+(C^+(m^n)) = m^n$.

Um \textbf{código livre de prefixo} é um código \emph{C'} em que $\nexists w_1^n, w_2^n \in W^+$ $|$ $w_1^n$ é \textbf{prefixo} de $w_2^n$, por exemplo, o conjunto de palavras código \emph{$W^+$} := $\{1, 01, 000, 001\}$ não possui nenhuma cadeia que é prefixo de outra dentro do conjunto. Códigos livres de prefixo podem ser \emph{decodificados instantaneamente}, ou seja, podemos decodificar uma palavra código sem precisar verificar o início da seguinte.

\begin{theorem}
Todo código \textbf{livre de prefixo} é \textbf{unicamente decodificável}.

\begin{proof}
Seja C um código livre de prefixo e $S_n$ = $s_1...s_n$ uma mensagem codificada por C. Vamos provar por indução que o teorema é verdadeiro para todo n $\in$ $\mathbb{Z+}$

\item \textbf{Casos base}: Quando n = 1, a mensagem S só possui uma palavra código, logo é unicamente decodificável. Se n = 2, então S possui uma palavra código $s_1$ que não pode ser prefixo de $s_2$ (pela própria definição de códigos livres de prefixo), o que claramente significa que S é unicamente decodificável.

\item \textbf{Passo indutivo}: Seja k $\in$ $\mathbb{Z+}$, e suponha por hipótese de indução que o teorema vale para n $\leq$ k. Como $S_{k+1}$ é livre de prefixo, existe um prefixo de $S_{k+1}$, $S_j$ = $s_1...s_j$ (com j $\leq$ k + 1) que é unicamente decodificável (dado que ela não pode ser prefixo de nenhuma outra). a mensagem $S'_{k+1}$ = $s_{j+1}...s_{k+1}$ ainda é uma concatenação decodificável e |$S'_{k+1}$| $\leq$ |$S_{k+1}$|, o que significa que por hipótese de indução $S'_{k+1}$ é unicamente decodificável. Como $S_{k+1}$ = $S_j$ $S'_{k+1}$, segue que $S_{k+1}$ é unicamente decodificável.
\end{proof}
\end{theorem}

\section{Relações fundamentais com a Teoria da Informação}
A codificação é comumente divida em duas componentes diferentes: \emph{modelo} e \emph{codificador}. O \emph{modelo} identifica a distribuição de probabilidade das mensagens baseado em sua semântica e estrutura. O \emph{codificador} toma vantagem de um possível \emph{bias} apontado pela modelagem, e usa uma estratégia gulosa em relação a probabilidade associada às mensagens para reduzir seu tamanho. (substituindo as mensagens que ocorrem com maior frequência por símbolos menores).

Desta forma, é evidente que os algoritmos de compressão sempre devem tomar vantagem de alguma distribuição de probabilidades "desbalanceada" sobre as mensagens para efetivamente reduzir o tamanho destas, portanto, a compressão é fortemente relacionada com a probabilidade. Nesta seção, vamos construir o embasamento teórico necessário para entender a relação entre as probabilidades associadas e o comprimento das mensagens, e consequentemente criar uma noção dos parâmetros que devem ser maximizados para alcançar uma codificação eficiente.

\subsection{Distribuição de Probabilidade e Esperança}
Dado um experimento e um espaço amostral $\Omega$, uma \textbf{variável aleatória} \emph{X} associa um número real a cada um dos possíveis resultados em $\Omega$. Em outras palavras, \emph{X} é uma função que mapeia os elementos do espaço amostral para números reais. Quando a imagem de \emph{X} pode assumir um número finito de valores, dizemos que \emph{X} é uma \textbf{variável aleatória discreta}.

Podemos descrever melhor uma variável aleatória, atribuindo probabilidades sobre os valores que esta pode assumir. Esses valores são atribuídos pela \textbf{função de densidade de probabilidade}, denotada por \emph{$p_X$}. Portanto, a probabilidade do evento \{ \emph{X} = \emph{x} \} é a função de distribuição de probabilidade aplicada a x, \emph{i.e}, \emph{$p_X(x)$}.
\begin{equation*}
p_X(x) = P(\{X = x\})
\end{equation*}

Note que, a variável aleatória pode assumir qualquer um dos valores no espaço amostral que possuem uma probabilidade $\emph{P} > 0$, portanto
\begin{equation*}
\sum_{x}^{}p_X(x) = 1.
\end{equation*}

O \textbf{valor esperado} (ou \textbf{esperança}) da variável aleatória \emph{X} é definido como
\begin{equation*}
\textbf{E}[\ X]\ = \sum_{x}^{} xp_X(x).
\end{equation*}

\subsection{Comprimento médio do código}
Seja \emph{p} a distribuição de probabilidade associada ao alfabeto de origem \emph{M}. Assuma que \emph{C} é um código tal que \emph{C(m)} = \emph{w}, definimos o \textbf{tamanho médio} de \emph{C} como:
\begin{equation*}
l_a (C) = \sum_{m \in M, w \in W^+}^{} p(m) l(w)
\end{equation*}

É possível notar que a função $l_a$ é delimitada pelo menor e maior comprimentos de palavras códigos em \emph{W}.  

Um código \emph{C} livre de prefixo é \textbf{ótimo} se $l_a(C)$ é mínimo, isto é, para qualquer código livre de prefixo \emph{C'} temos que
\begin{equation*}
l_a(C) \leq l_a(C')
\end{equation*}

\subsection{Entropia}
A \textbf{Entropia de Shannon} aplica as noções de Entropia física (que representa a aleatoriedade de um sistema) à Teoria da Informação. Dado um sistema \emph{S}, um estado \emph{s} $\in$ \emph{S} e a função \emph{p} sendo a distribuição de probabilidade associada a \emph{S}, definimos \textbf{Entropia} como:
\begin{equation*}
H(S) = \sum_{s \in S}^{} p(s) \log_2 \frac{1}{p(s)}
\end{equation*}

Podemos notar através desta definição que a Entropia de um sistema é inversamente proporcional a sua função de distribuição de probabilidade, em outras palavras, quanto mais "imprevisível" (no sentido de ter probabilidades bem distribuídas entre os seus diferentes estados) um sistema, maior a sua entropia.
A mesma definição pode ser aplicada a Teoria da Informação, "substituindo" o estado \emph{s} pela mensagem \emph{m}.

% --- Guardando para exemplo
% A formatação das referências bibliográficas conforme as regras da ABNT são um
% dos principais objetivos do \abnTeX. Consulte os manuais
% \citeonline{abntex2cite} e \citeonline{abntex2cite-alf} para obter informações
% sobre como utilizar as referências bibliográficas.

% %-
% \subsection{Acentuação de referências bibliográficas}
% %-

% Normalmente não há problemas em usar caracteres acentuados em arquivos
% bibliográficos (\texttt{*.bib}). Na~\autoref{tabela-acentos} você encontra alguns exemplos das conversões mais importantes. Preste atenção especial para `ç' e `í'
% que devem estar envoltos em chaves. A regra geral é sempre usar a acentuação
% neste modo quando houver conversão para letras maiúsculas.

% \begin{table}[htbp]
% \caption{Tabela de conversão de acentuação.}
% \label{tabela-acentos}

% \begin{center}
% \begin{tabular}{ll}\hline\hline
% acento & \textsf{bibtex}\\
% à á ã & \verb+\`a+ \verb+\'a+ \verb+\~a+\\
% í & \verb+{\'\i}+\\
% ç & \verb+{\c c}+\\
% \hline\hline
% \end{tabular}
% \end{center}
% \end{table}


% ---
% \section{Deu pau em algo?}
% ---

% Consulte a FAQ com perguntas frequentes e comuns no portal do \abnTeX:
% \url{https://code.google.com/p/abntex2/wiki/FAQ}.

% Inscreva-se no grupo de usuários \LaTeX:
% \url{http://groups.google.com/group/latex-br}, tire suas dúvidas e ajude a galera se tiver tudo certo.


