% ---
\chapter{Conceitos e definições fundamentais em compressão de dados}
% ---
Este capítulo apresenta algumas definições e conceitos fundametais para o entendimento das técnicas de compressão que serão discutidas em capítulos posteriores.

\section{Código}

Um \textbf{código} \emph{C} mapeia uma \textbf{mensagem} \emph{m} $\in$ \emph{M} para uma cadeia de \textbf{palavras código} em \emph{$W^+$},onde \emph{M} é chamado \textbf{alfabeto de origem} e \emph{$W^+$} \textbf{alfabeto de palavras código}. Vamos utilizar a notação $\emph{A}^+$ para se referir ao conjunto que contém todas as cadeias de \emph{A}, \emph{i.e}, $\emph{A}^+ = \bigcup_{i \geq 1}^{}A^{i} : A^i = (a_1,...,a_i), a \in A $. Deste modo, podemos representar um código como uma função \textbf{\emph{C} : $\emph{M} \rightarrow \emph{W}^+$}.  O \textbf{comprimento} da palavra código \emph{w}, definido pela função \emph{l}(\emph{w}), representa o número de bits de \emph{w}

Os elementos dos alfabetos de origem e de palavras código podem ter um comprimento fixo ou variável. Códigos nos quais os alfabetos possuem um comprimento fixo são chamados de \textbf{códigos de comprimento fixo}, enquanto os que possuem alfabetos de comprimento variáveis são chamados \textbf{códigos de comprimento variável}. Provavelmente o exemplo mais conhecido de código de comprimento fixo seja código ASCII, que mapeia 64 símbolos alfa-numéricos (ou 256 em sua versão estendida) para palavras código de 8 bits. Todavia, a compressão de dados utiliza apenas códigos de comprimento variável, mas especificamente códigos que variam o comprimento de acordo com a probabilidade associada à mensagem (o tema será melhor detalhado em seções posteriores). 

\subsection{Códigos unicamente decodificáveis e livres de prefixo}
Um código é \textbf{distinto} se pode ser representado como uma função \textbf{bijetora}, i.e, $\forall$ $m_1$, $m_2$ $\in$ M, \emph{C($m_1$)} $\neq$ \emph{C($m_2$)}. Um código é dito \textbf{unicamente decodificável} quando \emph{C(m)} = $w$ $\leftrightarrow$ \emph{$C^{-1}(w)$} = \emph{m}, com \emph{m} $\in$ \emph{M} e \emph{$w$} $\in$ $W^+$.

Vamos definir \emph{$C^+$} como a \textbf{codificação} correspondente ao código \emph{C}, tal que $C^+(m^n) = C(m_1)C(m_2)...C(m_n) : m^n = m_1m_2...m_n$, \emph{i.e}, \emph{$C^+ : M^+ \rightarrow W^+$}. A função de \textbf{decodificação} \emph{$D^+ : W^+ \rightarrow M^+$} se refere a operação inversa da codificação, de modo que dado um código \textbf{unicamente decodificável} \emph{C}, $D^+(C^+(m^n)) = m^n$.

Um \textbf{código livre de prefixo} é um código \emph{C'} em que $\nexists w_1, w_2 \in W^+$ $|$ $w_1$ é \textbf{prefixo} de $w_2$, por exemplo, o conjunto de palavras código \emph{$W^+$} := $\{1, 01, 000, 001\}$ não possui nenhuma cadeia que é prefixo de outra dentro do conjunto. Códigos livres de prefixo podem ser \emph{decodificados instantaneamente}, ou seja, podemos decodificar uma palavra código sem precisar verificar o início da seguinte.

Um código livre de prefixo pode ser modelado por uma arvore binária. Imagine que cada mensagem $\emph{m} \in \emph{M}$ é uma folha. A palavra código \emph{C'(m)} é o caminho \emph{p} da raiz até a folha correspondente a \emph{m}, de maneira em que, para cada nó percorrido concatene um bit à \emph{p} (0 quando o nó está à esquerda e 1 quando está à direita). Chamamos tal árvore de \textbf{árvore do código livre de prefixo}.

\section{Relações fundamentais com a Teoria da Informação}
A codificação é comumente divida em duas componentes diferentes: \emph{modelo} e \emph{codificador}. O \emph{modelo} identifica a distribuição de probabilidade das mensagens baseado em sua semântica e estrutura. O \emph{codificador} toma vantagem de um possível \emph{bias} apontado pela modelagem, e usa uma estratégia gulosa em relação a probabilidade associada às mensagens para reduzir seu tamanho. (substituindo as mensagens que ocorrem com maior frequência por símbolos menores).

Desta forma, é evidente que os algoritmos de compressão sempre devem tomar vantagem de alguma distribuição de probabilidades "desbalanceada" sobre as mensagens para efetivamente reduzir o tamanho destas, portanto, a compressão é fortemente relacionada com a probabilidade. Nesta seção, vamos construir o embasamento teórico necessário para entender a relação entre as probabilidades associadas e o comprimento das mensagens, e consequentemente criar uma noção dos parâmetros que devem ser maximizados para alcançar uma codificação eficiente.

\subsection{Distribuição de Probabilidade e Esperança}
Dado um experimento e um espaço amostral $\Omega$, uma \textbf{variável aleatória} \emph{X} associa um número real a cada um dos possíveis resultados em $\Omega$. Em outras palavras, \emph{X} é uma função que mapeia os elementos do espaço amostral para números reais. Quando a imagem de \emph{X} pode assumir um número finito de valores, dizemos que \emph{X} é uma \textbf{variável aleatória discreta}.

Podemos descrever melhor uma variável aleatória, atribuindo probabilidades sobre os valores que esta pode assumir. Esses valores são atribuídos pela \textbf{função de densidade de probabilidade}, denotada por \emph{$p_X$}. Portanto, a probabilidade do evento \{ \emph{X} = \emph{x} \} é a função de distribuição de probabilidade aplicada a x, \emph{i.e}, \emph{$p_X(x)$}.
\begin{equation} \label{eq:dist_prob_def}
p_X(x) = P(\{X = x\})
\end{equation}

Note que, a variável aleatória pode assumir qualquer um dos valores no espaço amostral que possuem uma probabilidade $\emph{P} > 0$, portanto
\begin{equation} \label{eq:dist_prob_sum}
\sum_{x}^{}p_X(x) = 1.
\end{equation}

O \textbf{valor esperado} (ou \textbf{esperança}) da variável aleatória \emph{X} é definido como
\begin{equation} \label{eq:exp_val}
\textbf{E}[\ X]\ = \sum_{x}^{} xp_X(x).
\end{equation}

\subsection{Comprimento médio do código}
Seja \emph{p} a distribuição de probabilidade associada ao alfabeto de origem \emph{M}. Assuma que \emph{C} é um código tal que \emph{C(m)} = \emph{w}, definimos o \textbf{tamanho médio} de \emph{C} como:
\begin{equation} \label{eq:code_len}
l_a (C) = \sum_{m \in M, w \in W^+}^{} p(m) l(w)
\end{equation}

Um código \emph{C} livre de prefixo é \textbf{ótimo} se $l_a(C)$ é mínimo, isto é, para qualquer código livre de prefixo \emph{C'} temos que
\begin{equation} \label{eq:code_len_optimal}
l_a(C) \leq l_a(C')
\end{equation}

\subsection{Entropia}
A \textbf{Entropia de Shannon} aplica as noções de Entropia física (que representa a aleatoriedade de um sistema) à Teoria da Informação. Dado um sistema \emph{S} e a função \emph{p} sendo a distribuição de probabilidade associada a \emph{S}, definimos \textbf{Entropia} como:
\begin{equation} \label{eq:entropy}
H(S) = \sum_{s \in S}^{} p(s) \log_2 \frac{1}{p(s)}
\end{equation}

Por esta definição temos que quanto menor o \emph{bias} da função de distribuição de probabilidade relacionada ao sistema, maior a sua entropia. Em outras palavras, a entropia de um sistema esta intimamente ligada a sua  "desordem". 

Shannon (incluir referencia papper do Shannon) aplica o mesmo conceito de entropia no contexto da teoria da informação, "substituindo" o conjunto de estados \emph{S} pelo conjunto de mensagem \emph{M}, isto é, \emph{M} é interpretado como um conjunto de possíveis mensagens, tendo como \emph{p(m)} a probabilidade de $m \in M$.
Baseado na mesma premissa, Shannon mede a informação contida em uma mensagem da seguinte forma:
\begin{equation} \label{label:info_quantity}
\emph{i(s)} = \log_2 \frac{1}{p(s)}.
\end{equation}

\subsection{Comprimento de Código e Entropia}
Nas secções anteriores, o comprimento médio de um código  foi definido em função da distribuição de probabilidade associada ao seu alfabeto de origem.
Da mesma forma, as noções de \textbf{Entropia} relacionada a um conjunto de mensagens, tem ligação direta com as probabilidades associadas a estas. 
A seguir, será mostrado como podemos relacionar o comprimento médio de um código a sua entropia através da \textbf{Desigualdade de Kraft-McMillan} , e por consequência estabelecer uma relação direta entre \textbf{a Entropia de um conjunto de mensagens e a otimalidade do código associada a estas mensagens}.

\begin{lemma}[Desigualdade de Kraft-McMillan] 
\textbf{Kraft.}. Para qualquer conjunto \emph{L} de comprimento códigos que satisfaça:
\begin{align*}
\sum_{l \in L}^{} 2^{-l} \leq 1.
\end{align*}
Existe ao menos um código livre de prefixo tal que, $|w_i|$ = $l_i$ ~,~$\forall w \in W^+$.

\textbf{Kraft-McMillan.} Para todo código binário unicamente decodificável \textbf{\emph{C} : $\emph{M} \rightarrow \emph{W}^+$} .
\begin{equation*}
\sum_{w \in W^+}^{}2^{-l(w)} \leq 1.
\end{equation*}


\begin{proof}

\item \textbf{Desigualdade de Kraft}
Sem perda de generalidade, suponha que os elementos de \emph{L} estão ordenados de maneira em que:
\begin{align*}
l_1 \leq l_2 \leq ... \leq l_n
\end{align*}
Agora vamos construir um código livre de prefixo em uma ordem crescente de tamanho, de maneira em que $l(w_i) = l_i$. Sabemos que um código é livre de prefixo se e somente se ,existe uma palavra código $w_j$  tal que nenhuma das palavras código anteriores $(w_1, w_2, w_{j-1})$ são prefixo de $w_j$.

Sem as restrições de prefixo, uma palavra código de tamanho $l_j$ poderia ser construída de $2^{l_j}$ maneiras diferentes. Com a restrição apresentada anteriormente, considerando uma palavra $w_k$ anterior a $w_j$ (i.e, $k < j$), existem $2^{l_j - l_k}$ possíveis palavras código em que $w_k$ seria um prefixo, e que portanto não podem pertencer ao código. Chamaremos tal conjunto de "palavras código proibidas". Vale notar que os elementos do conjunto de palavras código proibidas são excludentes entre si, pois se duas palavras código menores que \emph{j} fossem prefixo da mesma palavra código, elas seriam prefixos entre si. 

Dito isto, podemos definir o tamanho do conjunto de palavras código proibida para $w_j$.
\begin{equation*}
\sum_{i=1}^{j-1} 2^{l_j - l_i}
\end{equation*}

A construção do código livre de prefixo é possível se e somente se, existir ao menos uma palavra código de tamanho $j > 1$ que não está contida no conjunto das palavras código proibidas.
\begin{equation*}
2^{l_j} > \sum_{i=1}^{j-1} 2^{l_j - l_i}
\end{equation*}

Como o domínio do problema apresentado está restrito aos inteiros não negativos, podemos afirmar que:
\begin{align*}
2^{l_j} > \sum_{i=1}^{j-1} 2^{l_j - l_i} &= 2^{l_j} \geq \sum_{i=1}^{j-1} 2^{l_j - l_i} + 1 \\
&= 2^{l_j} \geq \sum_{i=1}^{j} 2^{l_j - l_i} \\
&= 1 \geq \sum_{i=1}^{j} 2^{-l_i} \\
&=  \sum_{i=1}^{j} 2^{-l_i} \leq 1
\end{align*}

Substituindo \emph{n} em \emph{j}, chegamos a desigualdade de Kraft.
\begin{equation*}
\sum_{l \in L}^{} 2^{-l} \leq 1.
\end{equation*}

Note que os argumentos utilizados para a construção da prova possuem dupla-equivalência, portando concluem a prova nos dois sentidos.

\item \textbf{Kraft-McMillan}
Suponha um código \textbf{unicamente decodificável} \emph{C} qualquer, e faça $l_{max}$ = $\max_{w}l(w)$.

Agora considere uma sequência de \emph{k} palavras código de \textbf{\emph{C} : $\emph{M} \rightarrow \emph{W}^+$} (onde \emph{k} é um inteiro positivo). Observe que:

\begin{align*}
(\sum_{w \in W^+}^{}2^{-l(w)})^k &= (\sum_{w_1}^{}2^{-l(w_1)}) \cdot (\sum_{w_2}^{}2^{-l(w_2)}) \cdot ... \cdot (\sum_{w_k}^{}2^{-l(w_k)}) \\
&= \sum_{w_1}^{} \sum_{w_2}^{} ... \sum_{w_k}^{} \prod_{j = 1}^{k} 2^{-l(w_j)} \\
&= \sum_{w_1,..., w_k}^{} 2^{- \sum_{j=1}^{k} l(w_j)} \\
&= \sum_{w_k}^{} 2^{-l(w^k)} \\
&= \sum_{j=1}^{k \cdot l_{max}} |\{w_k | l(w_k) = j\}| \cdot 2^{-j}.
\end{align*}

Sabemos que existem $2^{j}$ palavras código de tamanho \emph{j}, isto é, $|\{w_k | l(w_k) = j\}| = 2^j$. Para que o código seja unicamente decodificável obtemos o seguinte limite superior:
\begin{equation*}
(\sum_{w \in W^+}^{}2^{-l(w)})^k \leq \sum_{j=1}^{k \cdot l_{max}} 2^j \cdot 2^{-j} = k \cdot l_{max}
\end{equation*}

Logo,
\begin{equation*}
\sum_{w \in W^+}^{}2^{-l(w)} \leq (k \cdot l_{max})^ \frac{1}{k}
\end{equation*}

Note que a desigualdade é valida para qualquer $k > 0$ inteiro. Aproximando \emph{k} ao infinito, obtemos a desigualdade de Kraft-McMillan.

\begin{equation*}
\sum_{w \in W^+}^{}2^{-l(w)} \leq \lim_{k\to\infty} (k \cdot l_{max})^ \frac{1}{k} = 1.
\end{equation*}

\end{proof}
\end{lemma}

\begin{lemma}[Entropia como limite inferior para o comprimento médio] Dado um conjunto de mensagens \emph{M} associado a uma distribuição de probabilidades \emph{p} e um código unicamente decodificável \emph{C}.
\begin{equation*}
H(M) \leq l_a(C)
\end{equation*}

\begin{proof}
Queremos provar que $H(M) - l_a(C) \leq 0$, dado que  $H(M) \leq l_a(C) \Leftrightarrow H(M) - l_a(C) \leq 0$.

Substituindo a equação~\ref{eq:entropy} em \emph{H(M)} e~\ref{eq:code_len} em \emph{$l_a(C)$}, temos:

\begin{align*}
H(M) - l_a(C) &= \sum_{m \in M}^{}p(s) \log_2 \frac{1}{p(m)}  - \sum_{m \in M, w \in W^+}^{}p(m) l(w) \\
&= \sum_{m \in M, w \in W^+}^{}p(m) \left(  \log_2 \frac{1}{p(m)} - l(w) \right) \\
&= \sum_{m \in M, w \in W^+}^{}p(m) \left(  \log_2 \frac{1}{p(m)} - \log_2 2^{l(w)} \right) \\
&= \sum_{m \in M, w \in W^+}^{}p(m) \log_2 \frac{2^{-l(w)}}{p(m)}
\end{align*}

A \textbf{Desigualdade de Jansen} afirma que se uma função \emph{f(x)} é côncava, então $\sum_{i}{}p_i~f(x_i) \leq f(\sum_{i}{}p_i~x_i)$. Como a função $\log_2$ é côncava, podemos aplicar a Desigualdade de Jansen ao resultado obtido anteriormente.

\begin{equation*}
\sum_{m \in M, w \in W^+}^{}p(m) \log_2 \frac{2^{-l(w)}}{p(m)}  \leq \log_2(\sum_{m \in M, w \in W^+}{}2^{-l(w)})
\end{equation*}

Agora aplicamos a desigualdade de Kraft-McMillan, e concluímos que:

\begin{equation*}
H(M) - l_a(C) \leq \log_2(\sum_{m \in M, w \in W^+}{}2^{-l(w)}) \Rightarrow H(M) - l_a(C) \leq 0.
\end{equation*}


\end{proof}

\end{lemma}

\begin{lemma}[Entropia como limite superior para o comprimento médio de um código livre de prefixo ótimo] Dado um conjunto de mensagens \emph{M} associado a uma distribuição de probabilidades \emph{p} e um código livre de prefixo ótimo \emph{C}.
\begin{equation*}
l_a(C) \leq H(M) + 1
\end{equation*}

\begin{proof}
Sem perda de generalidade, para cada mensagem $m \in M$ faça \emph{l(m)} = $\left \lceil{\log_2 \frac{1}{p(m)} }\right \rceil $. Temos que:
\begin{align*}
\sum_{m \in M}^{} 2^{-l(m)} &= \sum_{m \in M}^{} 2^{-\left \lceil{\log_2 \frac{1}{p(m)} }\right \rceil} \\
&\leq \sum_{m \in M}^{} 2^{-{\log_2 \frac{1}{p(m)} }} \\
&= \sum_{m \in M}^{} p(m) \\
&= 1
\end{align*}

De acordo com a desigualdade de Kraft-McMillan existe um código livre de prefixo \emph{C'}  com palavras código de tamanho \emph{l(m)}, portanto:
\begin{align*}
l_a(C') &=  \sum_{m \in M', w \in W'^+}^{}p(m) l(w) \\
&=  \sum_{m \in M', w \in W'^+}^{}p(m) \left \lceil{\log_2 \frac{1}{p(m)} }\right \rceil \\
&\leq \sum_{m \in M', w \in W'^+}^{}p(m) (1 + \log_2 \frac{1}{p(m)}) \\
&= 1 +  \sum_{m \in M', w \in W'^+}^{}p(m) \log_2 \frac{1}{p(m)} \\
&= 1 + H(M)
\end{align*}

Pela definição de código livre de prefixo ótimo, $l_a(C) \leq l_a(C')$, isto é:
\begin{equation*}
l_a(C) \leq H(M) + 1 
\end{equation*}
\end{proof}
\end{lemma}


% ------------------- End of chapter 1 -----------------------------

% ---
\chapter{Algoritmos de compressão sem perda}
% ---

Baseado no conteúdo apresentado no capítulo anterior, os algoritmos de compressão podem ser vistos como um ramo da teoria da informação e seu objetivo primário é minimizar a quantidade de dados necessários para representar uma determinada informação.

Os algoritmos de compressão podem ser categorizadas em duas diferentes classes: os de compressão \textbf{com perda} e \textbf{sem perda}. Os \textbf{algoritmos de compressão sem perda} admitem uma baixa porcentagem de perda de informações durante a codificação para obter maior performance, muito uteis na transmissão de dados em streaming por exemplo. Nos \textbf{algoritmos de compressão com perda} o processo de codificação deve ser capaz de recuperar os dados em sua totalidade, geralmente utilizados em casos onde não pode haver perda de informações (como por exemplo, compressão de arquivos de texto).

Os algoritmos de compressão ser perda podem possuem dois principais métodos de implementação, codificação \text{baseada em entropia} e \textbf{baseada em dicionários}. Codificação baseada em entropia 

\section{Código de Huffman}
O \textbf{algoritmo de Huffman} (desenvolvido por David Huffman em 1952) é um dos componentes mais utilizados em algoritmos de compressão sem perda, servindo como base para algoritmos como o Deflate (utilizado amplamente na web).
Os códigos gerados a partir do algoritmos de Huffman são chamados \textbf{Códigos de Huffman}.

O código de Huffman é descrito em termos de como ele gera uma árvore de código livre de prefixo. Considere o conjunto de mensagens \emph{M}, com $p_i$ sendo a probabilidade associada a $m_i$

\begin{algorithm}[H]
\caption{Algoritmo de Huffman} \label{alg:huff}
\begin{algorithmic}
	\State $Forest \gets \emph{[]}$\\
	\ForAll{$m_i \in M$} \Comment{Inicializando floresta}
		\State $T \gets newTree()$
		\State $node \gets newNode()$
		\State $node.weight \gets p_i$ \Comment{$w_i = p_i$}
		\State $T.root \gets node$
		\State $Forest.append(T)$ \Comment{Adiciona um nova arvore mna floresta}
	\EndFor \\
	
	\While{$Forest.size > 1$}
		\State $T1 \gets ExtractMin(Forest)$ \Comment{Retorna a árvore cuja raiz é mínima, e retira da floresta}
		\State $T2 \gets ExtractMin(Forest)$
		\State $HTree \gets newTree()$
		\State $HTree.root \gets newNode()$ \\
		\State $HTree.root.left \gets T1.root$
		\State $HTree.root.right \gets T2.root$
		\State $HTree.root.weight \gets T1.root.weight + T2.root.weight$
		\State Forest.append(HTree) 
	\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Análise Assintótica}
Seja \emph{n} o tamanho do conjunto de mensagens \emph{M}. Para que o algoritmo percorra toda a floresta, formada por uma árvore para cada $\emph{m} \in \emph{M}$, serão necessárias \emph{n} interações.
Considerando que as funções \emph{ExtractMin()} e \emph{.append()} foram construídas a partir de uma fila de prioridades de \textbf{heap}, o algoritmo será executado em  $O(n \log_2 n)$.

\subsection{Corretude}
O teorema a seguir (escrito por Huffman) mostra a principal propriedade do Algoritmo de Huffman, os códigos de Huffman são códigos ótimos e livres de prefixo.


\begin{lemma} \label{lemma:dist_prob_avg_size} Seja \emph{C} um código ótimo livre de prefixo, com $\{ p_1, p_2,..., p_n\}$ sendo a distribuição de probabilidades associada ao código. Se $p_i > p_j$ então $l(w_i) \leq l(w_j)$

\begin{proof} 
Assuma que $l(c_i) > l(c_j)$. Agora vamos construi um novo código \emph{C'}, trocando $w_i$ por $w_j$. Dado $l_a$ como o comprimento médio do código \emph{C}, o código \emph{C'} terá o seguinte comprimento:
\begin{align*}
l'_a &= l_a + p_j(l(w_i) - l(w_j)) + p_i(l(w_j) - l(w_i)) \\
&= l_a + (p_j - p_i)(l(w_i) - l(w_j)) 
\end{align*}

Pelas suposições feitas anteriormente o termo $(p_j - p_i)(l(w_i) - l(w_j))$ seria negativo, contradizendo o fato do código \emph{C} ser um código ótimo e livre de prefixo (pois neste caso $l'_a > l_a$).

\textbf{Nota*} : Perceba que em uma árvore de Huffman, o tamanho da palavra código $w_i$ também representa seu nível na árvore.
\end{proof}
\end{lemma}

\begin{theorem} O algoritmo de Huffman gera um código ótimo livre de prefixo.
\begin{proof}
A prova se dará por indução sob o número de mensagens pertencentes ao código. Vamos mostrar que se o Algoritmo de Huffman gera um código livre de prefixo ótimo para qualquer distribuição de probabilidades com \emph{n} mensagens, então o mesmo ocorre para \emph{n + 1} mensagens.

\item \textbf{Caso Base.} Para n = 2 o teorema é trivialmente satisfeito considerando um código que atribui um bit pra cada mensagem do código.

\item \textbf{Passo indutivo}. Pelo lema \ref{lemma:dist_prob_avg_size} sabemos que as menores probabilidades estão nos menores níveis da árvore de Huffman (por ser uma árvore completa binário, o seu menor nível deve possuir ao menos dois nós). Por estes nós possuiriam o mesmo tamanho, podemos muda-los de posição sem afetar o tamanho médio do código, concluindo assim que estes são nós \textbf{irmãos}.\\
Agora defina um conjunto de mensagens \emph{M} de tamanho \emph{n + 1} onde T é a árvore de prefixo ótima construída a partir do Algoritmo de Huffman aplicado em \emph{M}. Vamos chamar os dois nós de menor probabilidade na árvore de \emph{x} e \emph{y} (que pelo argumento anterior, são nós irmãos). Vamos construir uma nova árvore \emph{T'} a partir de \emph{T} removendo os nós \emph{x} e \emph{y}, fazendo assim que o pai destes nós, que chamaremos de \emph{z}, seja o de menor probabilidade (de acordo com a definição do Algoritmo de Huffman, $p_z = p_y + p_x$). Considere \emph{k} como a profundidade de \emph{z}, temos:

\begin{align*}
l_a(T) &= l_a(T') + p_x(k + 1) + py(k + 1) - p_z k \\
&= l_a(T') + p_x + p_y
\end{align*}

Sabemos pela hipótese de indução que $l_a(T')$ é mínimo, pois \emph{T'} tem o tamanho \emph{n} e foi gerada pelo algoritmo de Huffman. Note que independente da ordem que forem inseridos, os nós \emph{x} e \emph{y} irão adicionar a constante $p_z = p_x + p_y$ no peso médio do código. Como $l_a(T')$ é mínimo para um conjunto de mensagens de tamanho \emph{n} e seu nó de menor peso tem distribuição de probabilidade $p_z$, $l_a(T)$ também é mínimo para o conjunto de mensagens \emph{M} e logo \emph{T} é ótimo e livre de prefixo. 
\end{proof}
\end{theorem}

% --- Guardando para exemplo
% A formatação das referências bibliográficas conforme as regras da ABNT são um
% dos principais objetivos do \abnTeX. Consulte os manuais
% \citeonline{abntex2cite} e \citeonline{abntex2cite-alf} para obter informações
% sobre como utilizar as referências bibliográficas.

% %-
% \subsection{Acentuação de referências bibliográficas}
% %-

% Normalmente não há problemas em usar caracteres acentuados em arquivos
% bibliográficos (\texttt{*.bib}). Na~\autoref{tabela-acentos} você encontra alguns exemplos das conversões mais importantes. Preste atenção especial para `ç' e `í'
% que devem estar envoltos em chaves. A regra geral é sempre usar a acentuação
% neste modo quando houver conversão para letras maiúsculas.

% \begin{table}[htbp]
% \caption{Tabela de conversão de acentuação.}
% \label{tabela-acentos}

% \begin{center}
% \begin{tabular}{ll}\hline\hline
% acento & \textsf{bibtex}\\
% à á ã & \verb+\`a+ \verb+\'a+ \verb+\~a+\\
% í & \verb+{\'\i}+\\
% ç & \verb+{\c c}+\\
% \hline\hline
% \end{tabular}
% \end{center}
% \end{table}


% ---
% \section{Deu pau em algo?}
% ---

% Consulte a FAQ com perguntas frequentes e comuns no portal do \abnTeX:
% \url{https://code.google.com/p/abntex2/wiki/FAQ}.

% Inscreva-se no grupo de usuários \LaTeX:
% \url{http://groups.google.com/group/latex-br}, tire suas dúvidas e ajude a galera se tiver tudo certo.


